{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"cfilt/iitb-english-hindi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['train']['translation'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_corpus(dtype='train', lang='hi'):\n",
    "    l_dataset = len(dataset[dtype])\n",
    "    for i in range(0, l_dataset, 1000):\n",
    "        yield [dataset[dtype][i + j][\"translation\"][lang] for j in range(min(1000,l_dataset-i))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hi_data in training_corpus(dtype='train',lang='hi'):\n",
    "    break;\n",
    "for en_data in training_corpus(dtype='train',lang='en'):\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens = old_tokenizer.tokenize(d[0])\n",
    "len(tokens),tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_VOCAB_SIZE = 75000\n",
    "HI_VOCAB_SIZE = 75000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_tokenizer = old_tokenizer.train_new_from_iterator(training_corpus('train'), HI_VOCAB_SIZE)\n",
    "en_tokenizer = old_tokenizer.train_new_from_iterator(training_corpus('train', lang='en'), EN_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer.save_pretrained(\"eng-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_tokenizer.save_pretrained(\"hindi-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens = hi_tokenizer.tokenize(hi_data[2])\n",
    "print(len(tokens),tokens)\n",
    "    hi_tokenizer.convert_tokens_to_string(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = en_tokenizer.tokenize(en_data[2])\n",
    "print(len(tokens),tokens)\n",
    "en_tokenizer.convert_tokens_to_string(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tokenizers from saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "hi_tokenizer = AutoTokenizer.from_pretrained(\"hindi-tokenizer\")\n",
    "en_tokenizer = AutoTokenizer.from_pretrained(\"eng-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_tokenizer.add_special_tokens({'pad_token': '[PAD]', 'cls_token': '<cls>', 'eos_token':'<eos>', 'bos_token' : '<s>'})\n",
    "\n",
    "en_tokenizer.add_special_tokens({'pad_token': '[PAD]', 'cls_token': '<cls>', 'eos_token':'<eos>', 'bos_token' : '<s>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "en_tokenizer._tokenizer.post_processor = TemplateProcessing(\n",
    "    single=en_tokenizer.bos_token + \" $A \" + en_tokenizer.eos_token,\n",
    "    special_tokens=[(en_tokenizer.eos_token, en_tokenizer.eos_token_id), (en_tokenizer.bos_token, en_tokenizer.bos_token_id)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sen = dataset['train']['translation'][1]['en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer.encode(en_sen, add_special_tokens = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translator - Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = list(range(0, 2))\n",
    "dataset['train'] = torch.utils.data.Subset(dataset['train'], subset)\n",
    "dataset['validation'] = torch.utils.data.Subset(dataset['validation'], subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 2\n",
    "train_loader = torch.utils.data.DataLoader(dataset['train'], batch_size=BS, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset['validation'], batch_size=BS, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset['test'], batch_size=BS, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in train_loader:\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = (hi_tokenizer(b['translation']['hi'], padding=True, truncation=True, return_tensors=\"pt\"),\n",
    "# en_tokenizer(b['translation']['en'], padding=True, truncation=True, return_tensors=\"pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss(ignore_index=en_tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(predictions, targets):\n",
    "    \"\"\"Compute our custom loss\"\"\"\n",
    "    predictions = predictions[:, :-1, :].contiguous()\n",
    "    targets = targets[:, 1:]\n",
    "\n",
    "    rearranged_output = predictions.view(predictions.shape[0]*predictions.shape[1], -1)\n",
    "    rearranged_target = targets.contiguous().view(-1)\n",
    "\n",
    "    loss = criterion(rearranged_output, rearranged_target)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "encoder_config = transformers.BertConfig(vocab_size=len(hi_tokenizer))\n",
    "decoder_config = transformers.BertConfig(vocab_size = len(en_tokenizer))\n",
    "\n",
    "config = transformers.EncoderDecoderConfig.from_encoder_decoder_configs(encoder_config, decoder_config)\n",
    "model = transformers.EncoderDecoderModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.decoder_start_token_id = en_tokenizer.cls_token_id\n",
    "model.config.pad_token_id = en_tokenizer.pad_token_id\n",
    "model.config.eos_token_id = en_tokenizer.eos_token_id\n",
    "model.config.bos_token_id = en_tokenizer.bos_token_id\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = transformers.AdamW(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    num_train_batches = len(train_loader)\n",
    "    for i, b in tqdm(enumerate(train_loader)):\n",
    "        if i%20==0:\n",
    "            print(i,end=' ')\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        hi_token = hi_tokenizer(b['translation']['hi'], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        en_token = en_tokenizer(b['translation']['en'], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "        hi_input = hi_token['input_ids'].to(device)\n",
    "        hi_masks = hi_token['attention_mask'].to(device)\n",
    "        \n",
    "        en_output = en_token['input_ids'].to(device)\n",
    "\n",
    "        out = model(input_ids=hi_input, attention_mask = hi_masks, labels=en_output)\n",
    "        prediction_scores = out[1]\n",
    "        predictions = F.log_softmax(prediction_scores, dim=-1)\n",
    "        loss = compute_loss(predictions, en_output)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / num_train_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(val_loader):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    num_valid_batches = len(val_loader)\n",
    "    for i, b in enumerate(val_loader):\n",
    "        if i%20==0:\n",
    "            print(i,end=' ')\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        hi_token = hi_tokenizer(b['translation']['hi'], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        en_token = en_tokenizer(b['translation']['en'], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "        def greedy_decoding(hi_token, en_token, device):\n",
    "            hi_input = hi_token['input_ids'].to(device)\n",
    "            hi_masks = hi_token['attention_mask'].to(device)\n",
    "\n",
    "            en_output = en_token['input_ids'].to(device)\n",
    "\n",
    "            BS = hi_input.shape[0]\n",
    "\n",
    "            pred_words = torch.tensor([[en_tokenizer.bos_token_id]]*BS)\n",
    "            dec_out = pred_words.to(device)\n",
    "            unfinished_seq = np.array([1]*BS)\n",
    "            \n",
    "            for i in range(en_output.shape[1]):\n",
    "                output = model(input_ids = hi_input, attention_mask = hi_masks, labels = dec_out )\n",
    "                pred_words = torch.argmax(output.logits, dim=-1)[:,-1:]    \n",
    "                pred_words[unfinished_seq==0,:] = en_tokenizer.pad_token_id\n",
    "                dec_out = torch.cat((dec_out,pred_words),dim=1)\n",
    "\n",
    "                print(pred_words)\n",
    "                unfinished_seq[(dec_out[:,-1] == en_tokenizer.eos_token_id).cpu().numpy()] = 0\n",
    "\n",
    "            predictions = F.log_softmax(output.logits, dim=-1)\n",
    "            \n",
    "            print(dec_out)\n",
    "            return compute_loss(predictions, en_output).item()\n",
    "        \n",
    "        epoch_loss += greedy_decoding(hi_token, en_token, device)\n",
    "\n",
    "    return (epoch_loss / num_valid_batches)\n",
    "\n",
    "eval_model(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(train_loader), len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "    train_epoch_loss = train_model(train_loader)\n",
    "    val_epoch_loss = eval_model(train_loader)\n",
    "    print(f\"\\n\\nepoch: {epoch}, train_loss: {train_epoch_loss}, val_loss: {val_epoch_loss}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./translate_hin_to_eng.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "encoder_config = transformers.BertConfig(vocab_size=len(hi_tokenizer))\n",
    "decoder_config = transformers.BertConfig(vocab_size = len(en_tokenizer))\n",
    "\n",
    "config = transformers.EncoderDecoderConfig.from_encoder_decoder_configs(encoder_config, decoder_config)\n",
    "model = transformers.EncoderDecoderModel(config)\n",
    "\n",
    "model.config.decoder_start_token_id = en_tokenizer.cls_token_id\n",
    "model.config.pad_token_id = en_tokenizer.pad_token_id\n",
    "model.config.eos_token_id = en_tokenizer.eos_token_id\n",
    "model.config.bos_token_id = en_tokenizer.bos_token_id\n",
    "\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(en_tokenizer.decode, d[1]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(input_ids=d[0]['input_ids'],\n",
    "                     attention_mask = d[0]['attention_mask'],\n",
    "                     labels = d[1]['input_ids']\n",
    "            )\n",
    "\n",
    "list(map(en_tokenizer.decode, torch.argmax(out.logits, dim=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = model.generate(input_ids = d[0]['input_ids'], decoder_start_token_id=en_tokenizer.cls_token_id)\n",
    "\n",
    "list(map(en_tokenizer.decode, output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# epoch_loss = 0\n",
    "\n",
    "\n",
    "# # optimizer.zero_grad()\n",
    "# out = model(input_ids=d[0]['input_ids'],\n",
    "#                          attention_mask = d[0]['attention_mask'],\n",
    "#                          labels = d[1]['input_ids'])\n",
    "\n",
    "# prediction_scores = out.logits\n",
    "# predictions = F.log_softmax(prediction_scores, dim=-1)\n",
    "# loss = compute_loss(predictions, d[1]['input_ids'])\n",
    "# epoch_loss += loss.item()\n",
    "\n",
    "# print(\"Mean validation loss:\", epoch_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(map(en_tokenizer.decode, torch.argmax(predictions,dim=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfinished_seq[(dec_out[:,-1] == en_tokenizer.eos_token_id).cpu().numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_tokenizer.decode(d[0]['input_ids'][1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in train_loader:\n",
    "    break;\n",
    "hi_token = hi_tokenizer(b['translation']['hi'], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "en_token = en_tokenizer(b['translation']['en'], padding=True, truncation=True, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## greedy decoding\n",
    "def greedy_decoding(hi_token, en_token):\n",
    "    BS = 2\n",
    "    model.eval()\n",
    "    pred_words = torch.tensor([[en_tokenizer.bos_token_id]]*BS)\n",
    "    dec_out = pred_words.to(device)\n",
    "\n",
    "    unfinished_seq = np.array([1]*BS)\n",
    "\n",
    "    for i in range(en_token['input_ids'].shape[1]):\n",
    "\n",
    "        output = model(input_ids = hi_token['input_ids'].to(device), attention_mask = hi_token['attention_mask'].to(device), labels = dec_out )\n",
    "        pred_words = torch.argmax(output.logits, dim=-1)[:,-1:]    \n",
    "        pred_words[unfinished_seq==0,:] = en_tokenizer.pad_token_id\n",
    "        dec_out = torch.cat((dec_out,pred_words),dim=1)\n",
    "\n",
    "        unfinished_seq[(dec_out[:,-1] == en_tokenizer.eos_token_id).cpu().numpy()] = 0\n",
    "\n",
    "\n",
    "    predictions = F.log_softmax(output.logits, dim=2)\n",
    "\n",
    "    loss = compute_loss(predictions, en_token['input_ids'].to(device))\n",
    "    print(loss.item())\n",
    "    print(list(map(en_tokenizer.decode, dec_out)), '\\n', list(map(en_tokenizer.decode, en_token['input_ids'])))\n",
    "\n",
    "greedy_decoding(hi_token, en_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## greedy decoding\n",
    "BS = 2\n",
    "model.eval()\n",
    "\n",
    "for b in train_loader:\n",
    "    break;\n",
    "hi_token = hi_tokenizer(b['translation']['hi'], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "en_token = en_tokenizer(b['translation']['en'], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "hi_input = hi_token['input_ids'].to(device)\n",
    "hi_masks = hi_token['attention_mask'].to(device)\n",
    "\n",
    "en_output = en_token['input_ids'].to(device)\n",
    "\n",
    "\n",
    "pred_words = torch.tensor([[en_tokenizer.bos_token_id]]*BS).to(device)\n",
    "dec_out = pred_words.to(device)\n",
    "\n",
    "unfinished_seq = np.array([1]*BS)\n",
    "\n",
    "for i in range(en_output.shape[0]):\n",
    "    while sum(unfinished_seq)>0:\n",
    "        output = model(input_ids = hi_input, labels = dec_out.to(device) )\n",
    "        pred_words = torch.argmax(output.logits, dim=-1)[:,-1:]\n",
    "        pred_words[unfinished_seq==0,:] = en_tokenizer.pad_token_id\n",
    "        dec_out = torch.cat((dec_out,pred_words),dim=1)\n",
    "\n",
    "        unfinished_seq[(dec_out[:,-1] == en_tokenizer.eos_token_id).cpu().numpy()] = 0\n",
    "        \n",
    "# loss = compute_loss(output.logits, en_output)\n",
    "# print(loss.item())\n",
    "list(map(en_tokenizer.decode, dec_out)), list(map(en_tokenizer.decode, en_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[1]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.logits.shape, d[1]['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
